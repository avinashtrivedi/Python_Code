{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<br><span style=\"font-size:x-large;\">CITS2402 Lab 2</span>\n",
    "\n",
    "# Covid Tests per Capita: Is the US Leading the World?\n",
    "\n",
    "![](trump-testing.jpg)\n",
    "\n",
    "<br>\n",
    "\n",
    "Former President Trump repeatedly stated that the US tests more than any other country in the world by far, and sometimes more than all the other countries put together.\n",
    "\n",
    "In this sequel to the *Fake News!* case study, we'll investigate the veracity of one aspect of these claims, the *per capita* test rates.\n",
    "\n",
    "In this lab we will also practise with:\n",
    "- The Data (Science) Lifecycle - data acquisition, cleaning and conversion (casting)\n",
    "- Working with (parsing) csv files\n",
    "- Working with text and numbers (casting)\n",
    "- Working with dictionaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Part 1\n",
    "\n",
    "### Data Acquisition\n",
    "\n",
    "#### *Our World in Data*\n",
    "\n",
    "In the videos for this case study we obtained data, originally from the European CDC, through *Our World in Data*: https://ourworldindata.org/.\n",
    "\n",
    "* Read the *[About](https://ourworldindata.org/about)* page to find out what this initiative is, what they hope to achieve, and who it is backed by. It is always important to know who is behind a data source in order to make an assessment about what degree of credibility to give the source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- Follow the link to `Health | Coronavirus Pandemic` and then to `Tests`. \n",
    "\n",
    "Note that the Data Scientists behind this site have provided a number of different ways of looking at and interpreting test rates. There is also a lot of background provided. \n",
    "\n",
    "* Scroll down to the section \"Our checklist for COVID-19 testing data\" and read through the ten items on the checklist.\n",
    "\n",
    "This site is terrific example of Data Science done well!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Per-capita testing\n",
    "\n",
    "Find the section entitled \"How many tests are performed each day\".\n",
    "\n",
    "Have a look at how the Map is presented, with the time slider that allows you to see snapshots over time.\n",
    "\n",
    "Then look at the Chart view and scroll the cursor over the chart.\n",
    "\n",
    "Again, these are great examples of interactive data presentation.\n",
    "\n",
    "Note: Consistent with many of the media sites, we will refer to these data in general terms as the \"per capita\" data to distinguish them from totals or per case data. More precisely, however, they are tests per 1000 people. That is, they are exactly 1000 times the per capita rate. The only reason for multiplying by 1000 is that its easier to read, say, 0.8 than 0.0008."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Downloading and inspecting the data\n",
    "\n",
    "- Go to `DOWNLOAD` and download the csv file `daily-tests-per-thousand-people-smoothed-7-day.csv`.\n",
    "\n",
    "- Open the file in your favourite spreadsheet software and take a look. You should see a list of daily results, much like we've been downloading in our Lockdown Tracker case study.\n",
    "\n",
    "- Have a look at the file in a terminal or your favourite text editor.\n",
    "\n",
    "You should see, as anticipated from the filename extension, that this is a *comma separated values (csv)* file: in each row, the *fields* are separated by commas. There is also a header line, indicating what the data in each field represents.\n",
    "\n",
    "Although various packages (modules) exist for reading in csv files, parsing data (breaking it into its constituent parts) is an important skill. In this lab we will therefore parse the file directly. The assessed questions therefore assume no additional packages are imported.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The current daily test file is currently over 2.5MB, and includes a lot of data since Trump's statements. We'll use a smaller version (430 KB), extracted on the 29th July 2020.\n",
    "\n",
    "This has been distributed with the lab sheet, and can be found in the file `daily-tests-per-thousand-people-smoothed-7-day-20200729.csv`.\n",
    "\n",
    "- Open this file in CoCalc.\n",
    "\n",
    "You should see that it is again in csv format with daily readings, but the way the data is reported has evolved over time. How does it compare with the latest download in terms of standardisation and consistency?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Reading in the data\n",
    "\n",
    "As usual, we'll start by setting up a constant with the path (in this case, its just the name) of the data file, so we don't need to keep typing it. We'll use the version of this file from the 29th July, distributed with the lab, so that we are all using the same file.\n",
    "\n",
    "You can access this file with:\n",
    "\n",
    "`DATA = \"daily-tests-per-thousand-people-smoothed-7-day-20200729.csv\"`\n",
    "\n",
    "Note that unlike the previous labs sheets, in this lab we won't continue to put empty cells in for you to complete your code in. You can create cells as you need them using the '+' button (or shortcuts 'a' and 'b')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA = \"daily-tests-per-thousand-people-smoothed-7-day-20200729.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "* Read and print out the first 5 lines of data. The output should start like this:\n",
    "\n",
    "```\n",
    "Entity,Code,Date,Daily tests per thousand people (7-day smoothed) (tests per thousand)\n",
    "\n",
    "Argentina,ARG,\"Feb 18, 2020\",0\n",
    "\n",
    "Argentina,ARG,\"Feb 19, 2020\",0\n",
    "\n",
    "Argentina,ARG,\"Feb 20, 2020\",0\n",
    "\n",
    "```\n",
    "\n",
    "In the first part of this lab we'll proceed on the assumption that this format is consistent throughout the file. (You can see by eyeballing the data that it isn't. In very large data files, however, you cannot always tell by eyeballing, and you have to test your assumptions using code. We'll do this in Part 2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity,Code,Date,Daily tests per thousand people (7-day smoothed) (tests per thousand)\n",
      "Argentina,ARG,\"Feb 18, 2020\",0\n",
      "Argentina,ARG,\"Feb 19, 2020\",0\n",
      "Argentina,ARG,\"Feb 20, 2020\",0\n",
      "Argentina,ARG,\"Feb 21, 2020\",0\n"
     ]
    }
   ],
   "source": [
    "#Read and print out the first 5 lines of data\n",
    "N = 5\n",
    "with open(DATA) as file:  # the a opens it in append mode\n",
    "    for i in range(N):\n",
    "        line = next(file).strip()\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data Cleaning and Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### From strings to lists\n",
    "\n",
    "* Read the first 5 lines again. This time use the `split` method to turn each line into a list before printing it out.\n",
    "\n",
    "Your output should start like this:\n",
    "```\n",
    "['Entity', 'Code', 'Date', 'Daily tests per thousand people (7-day smoothed) (tests per thousand)\\n']\n",
    "['Argentina', 'ARG', '\"Feb 18', ' 2020\"', '0\\n']\n",
    "['Argentina', 'ARG', '\"Feb 19', ' 2020\"', '0\\n']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Entity', 'Code', 'Date', 'Daily tests per thousand people (7-day smoothed) (tests per thousand)\\n']\n",
      "['Argentina', 'ARG', '\"Feb 18', ' 2020\"', '0\\n']\n",
      "['Argentina', 'ARG', '\"Feb 19', ' 2020\"', '0\\n']\n",
      "['Argentina', 'ARG', '\"Feb 20', ' 2020\"', '0\\n']\n",
      "['Argentina', 'ARG', '\"Feb 21', ' 2020\"', '0\\n']\n"
     ]
    }
   ],
   "source": [
    "#splitting the file lines\n",
    "\n",
    "N=5\n",
    "with open(DATA)as file:\n",
    "    for i in range(N):\n",
    "        line = next(file).split(\",\")\n",
    "        print(line)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Notice that we still have the newline character in the last item.\n",
    "\n",
    "* Use the `strip` function to remove whitespace before splitting the lines.\n",
    "\n",
    "Your output should now start like this:\n",
    "```\n",
    "['Entity', 'Code', 'Date', 'Daily tests per thousand people (7-day smoothed) (tests per thousand)']\n",
    "['Argentina', 'ARG', '\"Feb 18', ' 2020\"', '0']\n",
    "['Argentina', 'ARG', '\"Feb 19', ' 2020\"', '0']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Entity', 'Code', 'Date', 'Daily tests per thousand people (7-day smoothed) (tests per thousand)']\n",
      "['Argentina', 'ARG', '\"Feb 18', ' 2020\"', '0']\n",
      "['Argentina', 'ARG', '\"Feb 19', ' 2020\"', '0']\n",
      "['Argentina', 'ARG', '\"Feb 20', ' 2020\"', '0']\n",
      "['Argentina', 'ARG', '\"Feb 21', ' 2020\"', '0']\n"
     ]
    }
   ],
   "source": [
    "#using strip function to remove newline character \n",
    "N=5\n",
    "with open(DATA)as file:\n",
    "    for i in range(N):\n",
    "        line = next(file).strip().split(\",\")\n",
    "        \n",
    "        print(line)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "You may have noticed another problem, caused by the fact that the dates include commas. In fact, you may conclude that with this date format, the choice of a comma as the *delimiter* (separator) was not a particularly good one, and an alternative such as tab separated (tsv) would have been better choice for these data.\n",
    "\n",
    "Nevertheless, it is not ambiguous (importantly), because commas that are not intended as delimiters only appear within double quotes. The 'user' or *parser* of the file (our code in this case - but it could be, say, your favourite spreadsheet software) is expected to take the quotes into account when splitting the lines. \n",
    "\n",
    "There are many ways to deal with dates, but for now, we can just remove the comma and the quotes, since neither provide us with any information. The fields within the quotes (month, day and year) can be distinguished by their order (the comma is just for human consumption) and the quotes are otherwise redundant since it is a text file and all the fields will be read as strings.\n",
    "\n",
    "Change your code so that it has a *preprocessing* step before it splits the lines. For each line after the header line your preprocessing step should:\n",
    "  * find the positions of the two double quotes\n",
    "  * extract the text and remove the quotes and the comma\n",
    "  * replace the old date with the new date without quotes or a comma\n",
    "  * throw a `ValueError` if the line doesn't have double quotes\n",
    "\n",
    "We'll break it down into steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "* Print each line (of the first 5, other than the header, and with the whitespace removed) followed by the indices of the two double quotes:\n",
    "```\n",
    "Argentina,ARG,\"Feb 18, 2020\",0\n",
    "14 27\n",
    "Argentina,ARG,\"Feb 19, 2020\",0\n",
    "14 27\n",
    "Argentina,ARG,\"Feb 20, 2020\",0\n",
    "14 27\n",
    "...\n",
    "```\n",
    "\n",
    "Tip: You can put the double quote character in a string by enclosing it in single quotes (`'\"'`).\n",
    "\n",
    "Hint: Find the description of the String methods in the Python Library documentation, and compare the `find` and `index` methods. Why would you choose one or the other?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Argentina,ARG,\"Feb 18, 2020\",0\n",
      "Argentina,ARG,\"Feb 19, 2020\",0\n",
      "Argentina,ARG,\"Feb 20, 2020\",0\n",
      "Argentina,ARG,\"Feb 21, 2020\",0\n",
      "Argentina,ARG,\"Feb 22, 2020\",0\n"
     ]
    }
   ],
   "source": [
    "N=5\n",
    "with open(DATA)as file:\n",
    "    next(file)\n",
    "    for i in range(N):\n",
    "        line = next(file).strip().split(\",\")\n",
    "        print(\",\".join(line))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "* Next, using the indices, print the line followed by the date string:\n",
    "\n",
    "```\n",
    "Argentina,ARG,\"Feb 18, 2020\",0\n",
    "\"Feb 18, 2020\"\n",
    "Argentina,ARG,\"Feb 19, 2020\",0\n",
    "\"Feb 19, 2020\"\n",
    "Argentina,ARG,\"Feb 20, 2020\",0\n",
    "\"Feb 20, 2020\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Argentina,ARG,\"Feb 18, 2020\",0\n",
      "\"Feb 18,  2020\"\n",
      "Argentina,ARG,\"Feb 19, 2020\",0\n",
      "\"Feb 19,  2020\"\n",
      "Argentina,ARG,\"Feb 20, 2020\",0\n",
      "\"Feb 20,  2020\"\n",
      "Argentina,ARG,\"Feb 21, 2020\",0\n",
      "\"Feb 21,  2020\"\n",
      "Argentina,ARG,\"Feb 22, 2020\",0\n",
      "\"Feb 22,  2020\"\n"
     ]
    }
   ],
   "source": [
    "N=5\n",
    "\n",
    "with open(DATA)as file:\n",
    "    next(file)\n",
    "    for i in range(N):\n",
    "        line = next(file).strip().split(\",\")\n",
    "        \n",
    "        \n",
    "        print(\",\".join(line))\n",
    "        print(line[2]+\",\",line[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "* Now do the same, except with the quotes and any commas removed from the date string:\n",
    "```Argentina,ARG,\"Feb 18, 2020\",0\n",
    "Feb 18 2020\n",
    "Argentina,ARG,\"Feb 19, 2020\",0\n",
    "Feb 19 2020\n",
    "Argentina,ARG,\"Feb 20, 2020\",0\n",
    "Feb 20 2020    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Argentina,ARG,\"Feb 18, 2020\",0\n",
      "Feb 18, 2020\n",
      "Argentina,ARG,\"Feb 19, 2020\",0\n",
      "Feb 19, 2020\n",
      "Argentina,ARG,\"Feb 20, 2020\",0\n",
      "Feb 20, 2020\n",
      "Argentina,ARG,\"Feb 21, 2020\",0\n",
      "Feb 21, 2020\n",
      "Argentina,ARG,\"Feb 22, 2020\",0\n",
      "Feb 22, 2020\n"
     ]
    }
   ],
   "source": [
    "N=5\n",
    "\n",
    "with open(DATA)as file:\n",
    "    next(file)\n",
    "    for i in range(N):\n",
    "        line = next(file).strip().split(\",\")\n",
    "        \n",
    "        \n",
    "           \n",
    "        print(\",\".join(line))\n",
    "        x=(line[2].replace('\"',''),line[3].replace('\"',''))\n",
    "        y = (','.join(x))\n",
    "        print(y)\n",
    "        \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "* Next, print the original lines with the old date field replaced by the cleaned date field:\n",
    "```\n",
    "Argentina,ARG,Feb 18 2020,0\n",
    "Argentina,ARG,Feb 19 2020,0\n",
    "Argentina,ARG,Feb 20 2020,0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Argentina,ARG,\"Feb 18, 2020\",0\n",
      "Argentina,ARG,\"Feb 19, 2020\",0\n",
      "Argentina,ARG,\"Feb 20, 2020\",0\n",
      "Argentina,ARG,\"Feb 21, 2020\",0\n",
      "Argentina,ARG,\"Feb 22, 2020\",0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "N=5\n",
    "\n",
    "with open(DATA)as file:\n",
    "    next(file)\n",
    "    for i in range(N):\n",
    "        line = next(file).strip()\n",
    "        r = line.find('\"\"')\n",
    "        t = line[r].replace('\"\"','')\n",
    "        print(line)\n",
    "        \n",
    "        \n",
    "           \n",
    "       \n",
    "       \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Checked Solution [1 lab mark] \n",
    "Now that we have this working, we don't want this preprocessing step 'muddying' up our code, so let's put it together in a separate function. We'll also clean up the header line. We'll do this by writing a function according to the following *specification*.\n",
    "\n",
    "* Write a function `clean (data_row)` that takes as its argument a string, and:\n",
    "  * strips whitespace characters from the ends\n",
    "  * if the string contains a double quote (`\"`), strips the _first pair_ of double quotes and any commas between them\n",
    "    * throws a ValueError if the first double quote is not paired with a second (this indicates an error in the data - you do not need to worry about what the error message is at this stage)\n",
    "  * if the string doesn't contain any double quotes (eg. the header line), it truncates the line from the space before the first opening parenthesis, `(`, onwards\n",
    "    * does not throw a ValueError if the line does not contain an opening parenthesis (we don't really care if an explanation in parentheses was in the header)\n",
    "\n",
    "If called with the following code:\n",
    "```\n",
    "with open(DATA,'r') as file:\n",
    "    for i in range(5):\n",
    "        print(clean(file.readline()))\n",
    "```\n",
    "the output should start like this:\n",
    "```\n",
    "Entity,Code,Date,Daily tests per thousand people\n",
    "Argentina,ARG,Feb 18 2020,0\n",
    "Argentina,ARG,Feb 19 2020,0\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9eb16c610624fcf5411262497793cc9",
     "grade": false,
     "grade_id": "cleanq",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def clean (data_row):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "clean",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_571/4291250196.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mDATA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"daily-tests-per-thousand-people-smoothed-7-day-20200729.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0massert_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Entity,Code,Date,Daily tests per thousand people\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0massert_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Argentina,ARG,Feb 18 2020,0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0massert_raises\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text\"text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_571/3371988202.py\u001b[0m in \u001b[0;36mclean\u001b[0;34m(data_row)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclean\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata_row\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from nose.tools import assert_equal, assert_raises, raises\n",
    "DATA = \"daily-tests-per-thousand-people-smoothed-7-day-20200729.csv\"\n",
    "with open(DATA,'r') as file:\n",
    "    assert_equal(clean(file.readline()), \"Entity,Code,Date,Daily tests per thousand people\")\n",
    "    assert_equal(clean(file.readline()), \"Argentina,ARG,Feb 18 2020,0\")\n",
    "assert_raises(ValueError, clean, 'text\"text')\n",
    "\n",
    "print(\"So far, so good on the practice tests. Remember there will be additional tests applied. You should test your code to ensure it meets the specification.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### From a file to a list\n",
    "\n",
    "Next, rather than printing the lines, store them all in a list (of lists).\n",
    "\n",
    "* Define a variable `data_lists` as an empty list (). Write code that cleans and splits the (entire) input into lists, and appends them to `data_lists`.\n",
    "\n",
    "For example, the following should print the first five rows as a list of lists:\n",
    "\n",
    "```\n",
    "print(data_lists[:5])\n",
    "\n",
    "[['Entity', 'Code', 'Date', 'Daily tests per thousand people'], ['Argentina', 'ARG', 'Feb 18 2020', '0'], ['Argentina', 'ARG', 'Feb 19 2020', '0'], ['Argentina', 'ARG', 'Feb 20 2020', '0'], ['Argentina', 'ARG', 'Feb 21 2020', '0']]\n",
    "```\n",
    "\n",
    "How many entries (lines of data) are there in the file?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Checking our cleaning so far\n",
    "\n",
    "We now have a tidy list of lists, each with the four fields. Or do we?\n",
    "\n",
    "With big data it may not be possible to manually look at every entry to see if we've accounted for every possibility. We should try to make our cleaning or preprocessing as general as possible so that we catch unexpected variations or bad data. \n",
    "\n",
    "In practice, we often have to make some assumptions about the data. However we should endeavour to test these.\n",
    "\n",
    "In this case, we've assumed that the patterns we see at the start of the file continue through the file. So let's check that assumption.\n",
    "\n",
    "* Write code that checks whether all your entries have 4 fields.\n",
    "\n",
    "If not, why not? What have we missed?\n",
    "\n",
    "Hint: Print out the first row (if there is one) where this is not true. Print out the number of that row, open the data file in CoCalc, and have a look at the data in that row. What do you find?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Part 2\n",
    "\n",
    "#### General vs Specific\n",
    "\n",
    "The variations you see in the data are not unusual - remember that this is the collated official government data, its not an 'exercise'. It is *exactly the kind of thing you would deal with as a working Data Scientist*.\n",
    "\n",
    "We'll need to come back to some differences in the content of the data, but for now let's focus on the formatting. Or, more precisely, *transforming* the data from the format in which it is provided to a format that is suitable for our use.\n",
    "\n",
    "* Write a new function `clean2()` to do more cleaning as necessary so that it is transformed into a list of lists, each with four fields.\n",
    "\n",
    "You should try to make your code as **general as possible**. This means that, rather than just adjust for the specific case, think about *patterns*. \n",
    "\n",
    "For example, we have seen that the data format uses double quotes around fields containing the delimiter (a comma in this case). If the data is not corrupt (which is an assumption) therefore, we would expect the double quotes to always occur in pairs.\n",
    "By focussing only on dates, we have been more *specific* than we need to be, so we may miss other cases. A more *general* solution will assume that the same pattern may occur in other fields.\n",
    "\n",
    "Ensure you re-test your code after any changes you make to ensure it satisfies the requirements. (You might find it useful to write them all down.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Type casting\n",
    "\n",
    "Finally, the last field, tests per 1000 people, should be a `float`.\n",
    "\n",
    "* Alter your code to change the tests ratio to a float.\n",
    "\n",
    "Your first few lines should now look like this:\n",
    "```\n",
    "[['Entity', 'Code', 'Date', 'Daily tests per thousand people'], ['Argentina', 'ARG', 'Feb 18 2020', 0.0], ['Argentina', 'ARG', 'Feb 19 2020', 0.0], ['Argentina', 'ARG', 'Feb 20 2020', 0.0], ['Argentina', 'ARG', 'Feb 21 2020', 0.0]]\n",
    "```\n",
    "\n",
    "Again, we could make an assumption that the fourth string is always able to be converted to a float, but its possible that somewhere in the file that is not true. Later we will deal with this using *Exceptions*. For now, it is good practice to do some checks before attempting to cast. Before casting, check that:\n",
    "* the fourth field is a non-empty string containing only decimal characters (as defined by Unicode) and decimal points\n",
    "\n",
    "Tip: You may find the string function `isdecimal()` useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Remember to run your own tests, not rely on my tests or my sample output. For example, I've included the first few lines of output (the file is too large to include them all) but as we've seen those lines may not be indicative of the file as a whole. For one thing, they are all cases where the daily test ratio is zero.\n",
    "\n",
    "Also, as always remember to check your functions with a \"clean\" kernel.\n",
    "\n",
    "Remember that a Data Scientist is like a detective - always thinking about what we could possibly have missed, and testing for it.\n",
    "\n",
    "Now let's put it all together in a more general function..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Checked Solution [2 marks]\n",
    "\n",
    "* Complete the function `get_cleaned_lists (filename)` so that it returns a list of lists, each containing the fields from the data file, with \n",
    "  * any pairs of quotes, and the commas within them, removed\n",
    "    * mismatched quotes should throw a ValueError\n",
    "  * any parentheses and text contained within them removed\n",
    "    * mismatched parentheses should throw a ValueError (note this is different to the earlier specification)\n",
    "  * any leading/trailing whitespace characters removed from all resulting fields\n",
    "  * the daily tests ratio as a float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5a9a5217a3eb70e2be0006cd3cfbcf3a",
     "grade": false,
     "grade_id": "d2098c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_cleaned_lists (filename):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "get_cleaned_lists_1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_571/1497816639.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0massert_equal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mDATA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"daily-tests-per-thousand-people-smoothed-7-day-20200729.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cleaned_lists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0massert_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Entity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Code'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Daily tests per thousand people'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0massert_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Argentina'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ARG'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Feb 18 2020'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_571/3274592706.py\u001b[0m in \u001b[0;36mget_cleaned_lists\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from nose.tools import assert_equal\n",
    "DATA = \"daily-tests-per-thousand-people-smoothed-7-day-20200729.csv\"\n",
    "data = get_cleaned_lists(DATA)\n",
    "assert_equal(data[0],['Entity', 'Code', 'Date', 'Daily tests per thousand people'])\n",
    "assert_equal(data[1],['Argentina', 'ARG', 'Feb 18 2020', 0.0])\n",
    "assert_equal(data[40],['Argentina', 'ARG', 'Mar 28 2020', 0.008])\n",
    "assert_equal(data[180],['Argentina tests performed', '', 'Mar 10 2020', 0.0])\n",
    "print(\"So far, so good. Remember there will be additional tests applied.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "get_cleaned_lists_2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# For marking use only.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Part 3\n",
    "\n",
    "### Data Presentation  - Using Dictionaries\n",
    "\n",
    "*For this part it is left to you to break down the task into subtasks and test them as you go.*\n",
    "\n",
    "We want to be able to access the daily test ratio for a country on a given date without using loops.\n",
    "\n",
    "Dictionaries provide *much* faster access to data by *hashing* the dictionary keys.\n",
    "\n",
    "* Store the daily tests data in a dictionary of dictionaries. The outer dictionary should use the countries as keys. The inner dictionary should use the dates as keys.\n",
    "\n",
    "For example, if my outer dictionary is called `country_dict`, then evaluating `country_dict[\"Australia\"]` should return:\n",
    "    \n",
    "```\n",
    "{'Mar 29 2020': 0.382,\n",
    " 'Mar 30 2020': 0.396,\n",
    " 'Mar 31 2020': 0.41,\n",
    " 'Apr 1 2020': 0.425,\n",
    " 'Apr 2 2020': 0.439,\n",
    " 'Apr 3 2020': 0.453,\n",
    " 'Apr 4 2020': 0.467,\n",
    " ...\n",
    "```\n",
    "and `country_dict[\"Australia\"][\"Jul 1 2020\"]` should return:\n",
    "```\n",
    "1.824\n",
    "```\n",
    "\n",
    "How many \"countries\" are there?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "* Print out each country, followed by its number of tests per 1000 people, on 1st July, 2020. [If the country doesn't have a reading for that day, it can simply be skipped.]\n",
    "\n",
    "It will be clear from this that some countries have more than one entry - for example, \"Poland\" and \"Poland people tested\" will show as two separate \"countries\". While we could easily clean these out, we haven't looked closely enough at the sources of the data to determine the reason for the different figures, and whether one is better than the other, to have grounds for choosing one over the other (you may wish to follow this up). Therefore we need to leave them all in for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Presenting rankings in a table\n",
    "\n",
    "* Finally, write a function `print_ranking(date)` that takes a date (as a string) and prints a table of testing results for that date, ranked from highest testing rate to the lowest.\n",
    "\n",
    "So, for example, `print_ranking(\"Jul 1 2020\")` will start as follows:\n",
    "    \n",
    "```\n",
    "Testing results for Jul 1 2020\n",
    "8.032 \t Luxembourg\n",
    "5.653 \t United Arab Emirates\n",
    "5.055 \t Bahrain\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Checking the news\n",
    "\n",
    "The Whitehouse statement cited in the introduction, above, was made on April 28th.\n",
    "\n",
    "What do you make of President Trump's statements *from a per capita perspective*? Was he correct?\n",
    "\n",
    "How does that compare with more recently?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In a May 11 _Rose Garden briefing_ President Trump stated:\n",
    "\n",
    "> We’re testing more people per capita than South Korea, the United Kingdom, France, Japan, Sweden, Finland, and many other countries — and, in some cases, combined.\n",
    "\n",
    "The BBC's May 15th article [Coronavirus: President Trump’s testing claims fact-checked](https://www.bbc.com/news/world-us-canada-52493073) \"fact-checks\" this claim (Claim One).\n",
    "\n",
    "* Modify your function to the signature `print_ranking(date, countries=[])` so that:\n",
    "  * if `countries` is omitted, it still prints the table for *all* countries reporting on that day\n",
    "  * if a list of countries is passed to the function, then it only prints the table for those countries\n",
    "\n",
    "Print the table for the US and those six countries on 11th May.\n",
    "\n",
    "Is the BBC's fact check for the 11th May borne out by these data?\n",
    "\n",
    "Can you think of a possible reason for these discrepancies? (Hint: Should \"we're testing\" be interpreted as a rate or as a cumulative total?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "On 22nd June Newsweek, in [Why Trump Is Both Right and Wrong About U.S. Coronavirus Testing Numbers](https://www.newsweek.com/why-trump-right-wrong-about-us-coronavirus-testing-1512472), compares the US with Russia, Spain, Germany and Portugal on cumulative per capita figures. \n",
    "\n",
    "How does this compare with the picture you get for the daily rate at this date?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Congratulations - you can now get a job as a fact checking journalist!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "&copy; Cara MacNish, Univeristy of Western Australia"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
