{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format\n",
    "* Monday10/04 12:00-1:15pm\n",
    "* review notebook1-8, hw1, discussion problems (if you have time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Basic pandas dataframe knowledge (hw1)\n",
    "* Understanding concepts:\n",
    "    * what are they? ie. MSE/R^2...\n",
    "    * what are they use for?\n",
    "    * when/why we should use those methods?\n",
    "* See explaination for HW1 q4.4\n",
    "* Dont forget to restart+rerun your notebook before submission!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week1\n",
    "Understand pdf, cdf and other measurements. How to create those distributions in python.\n",
    "#### PDF\n",
    "https://amsi.org.au/ESA_Senior_Years/SeniorTopic4/4e/4e_2content_3.html\n",
    "\n",
    "The probability density function (pdf) f(x) is defined as the derivative of the cdf F(x)\n",
    "> $\\displaystyle \\mathbb{f}(x) = \\frac{d}{dx}F(x)$\n",
    "\n",
    "#### CDF\n",
    "Can be used with both continuous and discrete distributions.\n",
    "\n",
    "- Integral up to given outcome $x$: Probability of being lesser or equal than $x$\n",
    "\n",
    "> $\\displaystyle \\mathrm{CDF}(x) = \\int_{-\\infty}^{x} p(t)\\,dt$\n",
    "#### Measurements\n",
    "Let $p(x)$ be the PDF.\n",
    "\n",
    "- Expectation value of $X$\n",
    "- Expectation value of any $f(X)$\n",
    "- Moments     \n",
    "- Central moments \n",
    "- Variance\n",
    "- Standard deviation\n",
    "- Normalized moments\n",
    "- Skewness\n",
    "- Kurtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week2\n",
    "Sampling on different distributions (week2/lec3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week3\n",
    "#### Dependence and correlation (Bivariate and multivariate Gaussian distribution)\n",
    "#### Least Squares\n",
    "https://www.investopedia.com/terms/l/least-squares-method.asp\n",
    "\n",
    "The least-squares method is a statistical procedure to find the best fit for a set of data points by minimizing the sum of the offsets or residuals of points from the plotted curve.\n",
    "\n",
    "#### Linear Regression\n",
    "Linear regression is a linear model, e.g. a model that assumes a linear relationship between the input variables (x) and the single output variable (y). More specifically, that y can be calculated from a linear combination of the input variables (x). Most of time takes coutinous variables as inputs.\n",
    "\n",
    "When there is a single input variable (x), the method is referred to as simple linear regression. When there are multiple input variables, literature from statistics often refers to the method as multiple linear regression.\n",
    "\n",
    "For example, in a simple regression problem (a single x and a single y), the form of the model would be:\n",
    "\n",
    "y = B0 + B1*x\n",
    "\n",
    "In higher dimensions when we have more than one input (x)\n",
    "#### Mean Squared Errors (MSE)\n",
    "https://vitalflux.com/mean-square-error-r-squared-which-one-to-use/#What_is_Mean_Squared_Error_MSE\n",
    "\n",
    "Mean squared error (MSE) is the average of sum of squared difference between actual value and the predicted or estimated value. \n",
    "<img src=https://miro.medium.com/max/386/1*zfMHgFXHk2X2nNa8idN_Hg.png style=\"width: 200px;\"/>\n",
    "The value of MSE is always positive or greater than zero. A value close to zero will represent better quality of the estimator / predictor (regression model). An MSE of zero (0) represents the fact that the predictor is a perfect predictor. \n",
    "#### R^2\n",
    "R-Squared is the ratio of Sum of Squares Regression (SSR) and Sum of Squares Total (SST). Sum of Squares Regression is amount of variance explained by the regression line. R-squared value is used to measure the goodness of fit. Greater the value of R-Squared, better is the regression model.\n",
    "\n",
    "#### Bias–variance tradeoff\n",
    "- Bias: the difference between the average prediction of our model and the correct value which we are trying to predict. \n",
    "\n",
    "- Variance: the variability of model prediction for a given data point or a value which tells us the spread of our data.\n",
    "In statistics and machine learning, the bias–variance tradeoff is the property of a model that the variance of the parameter estimated across samples can be reduced by increasing the bias in the estimated parameters.\n",
    "\n",
    "<img src=https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/images/bias_variance/bullseye.png alt=\"image info\" style=\"width: 500px;\"/>\n",
    "\n",
    "High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).\n",
    "\n",
    "The variance is an error from sensitivity to small fluctuations in the training set. High variance may result from an algorithm modeling the random noise in the training data (overfitting)\n",
    "\n",
    "#### Underfitting & Overfitting\n",
    "\n",
    "Underfitting is a scenario in data science where a data model is unable to *capture* the relationship between the input and output variables accurately, generating a high error rate on both the training set and unseen data.\n",
    "\n",
    "Overfitting is the opposite of underfitting, occurring when the model has been overtrained or when it contains too much complexity, resulting in high error rates on test data. Unable to *generate* for other data.\n",
    "\n",
    "\n",
    "<img src=https://www.datarobot.com/wp-content/uploads/2018/03/Screen-Shot-2018-03-22-at-11.22.15-AM-e1526498075543.png alt=\"image info\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week4&5\n",
    "#### LASSO VS RIDGE\n",
    "https://zawar-ahmed.medium.com/comparing-linear-regression-models-lasso-vs-ridge-60587ff5a5aa\n",
    "https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lasso(L1 regularization)**\n",
    "\n",
    "LASSO lowers the size of the coefficients and leads to some features having a coefficient of 0, essentially dropping it from the model.\n",
    "\n",
    "The LASSO, however, does not do well when you have a low number of features because it may drop some of them to keep to its constraint, but that feature may have a decent effect on the prediction. It also does not do well with features that are highly correlated and one(or all) of them may be dropped when they do have an effect on the model when looked at together.\n",
    "\n",
    "**Ridge Regression(L2 regularization)**\n",
    "\n",
    "The Ridge Regression aims to lower the sizes of the coefficients to avoid over-fitting, but it does not drop any of the coefficients to zero. \n",
    "\n",
    "It performs better in cases where there may be high multi-colinearity, or high correlation between certain features. This is because it reduces variance in exchange for bias. You also need to make sure that the number of features is less than the number of observations before using Ridge Regression because it does not drop features and in that case may lead to bad predictions.\n",
    "\n",
    "#### PCA\n",
    "https://towardsdatascience.com/principal-component-analysis-intro-61f236064b38\n",
    "\n",
    "\n",
    "PCA(Principal Component Analysis) is a method used to reduce number of variables in your data by extracting important one from a large pool. In other words, we find low-dimensional representation of high-dimensional data.It reduces the dimension of your data with the aim of retaining as much information as possible. So, it somehow helps avoid issues like over-fitting in high dimensional space\n",
    "\n",
    "This method combines highly correlated variables together to form a smaller number of an artificial set of variables which is called “principal components” that account for most variance in the data.\n",
    "\n",
    "The eigenvectors of largest eigenvalues capture the most variance\n",
    "\n",
    "> If keeping only $k<D$ eigenvectors, the best approximation is taking the first $k$ PCs\n",
    "><br/><br/>\n",
    "> $\\displaystyle  C \\approx \\sum_{i=1}^k\\ \\lambda_i\\left({e}_i\\,{e}_i^T\\right) =  E_k\\Lambda_k E_k^T$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### correlation matrix\n",
    "A correlation matrix is a table showing correlation coefficients between sets of variables.\n",
    "\n",
    "https://www.statisticshowto.com/correlation-matrix/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
